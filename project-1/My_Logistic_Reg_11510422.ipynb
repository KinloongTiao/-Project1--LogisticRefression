{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood approach for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Author: Jaylen\\; Diao\\;刁金龙 \\quad Stu No: 11510422  \\quad   Date: 2019/04/01  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the logistic regression for two-class problemp: Assuming that  \n",
    "\n",
    "$\n",
    "\\qquad {\\bf w} = (w_0,w_1,...,w_d )^T ,\n",
    "$\n",
    "\n",
    "and x includes the constant 1 in its first component\n",
    "\n",
    "\n",
    "$ \\qquad\n",
    "Pr(y=1|{\\bf X}={\\bf x}) = \\frac{e^{({\\bf w}^{T}x)}}{1+e^{({\\bf w}^{T}x)}}\n",
    "$;\n",
    "\n",
    "$ \\qquad\n",
    "Pr(y=0|{\\bf X}={\\bf x}) = \\frac{1}{1+e^{({\\bf w}^{T}x)}}.\n",
    "$\n",
    "\n",
    "By a logit transformation, $ \\log[\\frac{p}{1− p}]$, we recover a linear regression model:\n",
    "\n",
    "$ \\qquad\n",
    "\\log \\frac{Pr(y=1|{\\bf X}={\\bf x})}{Pr(y=0|{\\bf X}={\\bf x})} = {\\bf w}^{T}x\n",
    "$\n",
    "\n",
    "The decision boundary is the set of points for which the above quantity is zero, and this\n",
    "is a hyperplane defined by $ {\\bf w}^{T}x = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Often, sigmoid function refers to the special case of the logistic function shown in the first figure and defined by the formula\n",
    "\n",
    "$ \\qquad\n",
    "\\sigma (a) = \\frac{1}{1+e^{(-a)}}\n",
    "$\n",
    "\n",
    "In general, a sigmoid function is monotonic, and has a first derivative which is bell shaped. A sigmoid function is constrained by 0 and 1 as ${\\displaystyle x\\rightarrow \\pm \\infty }$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def sigmoid(x):\n",
    "    # 直接返回sigmoid函数\n",
    "    return 1. / (1. + np.exp(-x))\n",
    " \n",
    "def plot_sigmoid():\n",
    "    # param:起点，终点，间距\n",
    "    x = np.arange(-8, 8, 0.2)\n",
    "    y = sigmoid(x)\n",
    "    \n",
    "    plt.vlines(0, 0, 0.5, colors = \"m\", linestyles = \"dashed\")\n",
    "    plt.hlines(0.5, -8, 0, colors = \"m\", linestyles = \"dashed\")\n",
    "    plt.hlines(0, -8, 8, colors = \"g\", linestyles = \"dashed\")\n",
    "    plt.hlines(1, -8, 8, colors = \"g\", linestyles = \"dashed\")\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "    \n",
    "%matplotlib inline\n",
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Probility Distribution \n",
    "\n",
    "Remember that we have used the maximum likelihood approach to interpret the linear\n",
    "regression. Now we apply the same method to two-class logistic regression. \n",
    "\n",
    "Given $\\Bbb D = (({\\bf x}_1,y_1),({\\bf x}_2,y_2),...,({\\bf x}_n,y_b)), {\\bf x}_i \\in {\\Bbb R}^d, y \\in \\lbrace0,1\\rbrace$\n",
    "\n",
    "$ \\qquad\n",
    "y_i \\sim Bernouli(\\sigma({\\bf w}^{T}x)))\n",
    "$\n",
    "\n",
    "For convenience, we write this distribution as \n",
    "  \n",
    "$\n",
    "\\qquad \\alpha_i = \\sigma({\\bf w}^T {\\bf x_i}) = p_{1}({\\bf x_i};{\\bf w}) = 1- p_{0}({\\bf x_i};{\\bf w})\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${Compute \\; \\alpha \\;Matrix \\\\\n",
    "输入参数w是d*1列向量，xdata是n*d矩阵} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = \n",
      " [[0.1]\n",
      " [0.2]\n",
      " [0.2]]\n",
      "data = \n",
      " [[1 2 3]\n",
      " [2 2 1]\n",
      " [1 3 1]]\n",
      "y = \n",
      " [[1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个样例\n",
    "w = np.array([0.1,0.2,0.2])\n",
    "w = np.mat(w)\n",
    "w = np.transpose(w)\n",
    "data = np.array([[1,2,3],[2,2,1],[1,3,1]])\n",
    "data = np.mat(data)\n",
    "y = np.array([1,0,1])\n",
    "y = np.mat(y)\n",
    "y = np.transpose(y)\n",
    "print(\"w = \\n\",w)\n",
    "print(\"data = \\n\", data)\n",
    "print(\"y = \\n\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.75026011],\n",
       "        [0.68997448],\n",
       "        [0.7109495 ]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import mat\n",
    "def getAlphaMatrix(w,xdata):\n",
    "    \n",
    "    wT_x = np.dot(np.transpose(w),np.transpose(xdata))\n",
    "\n",
    "    return np.exp(np.transpose(wT_x))/(1+np.exp(np.transpose(wT_x)))\n",
    "\n",
    "getAlphaMatrix(w,data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood function\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf w_{MLE}} \\in \\underbrace{argmax}_{\\bf w} \\;\\; P({\\Bbb D}|{\\bf w})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Denote the probability $ P r (y = k|{\\bf X} = {\\bf x}) = p_k ({\\bf x};{\\bf w}), k = $ 0 or 1. The likelihood function is defined by\n",
    "\\begin{equation}\n",
    "L({\\bf w})=\\prod_{i=1}^n p_{y_i}({\\bf x_i};{\\bf w}) = \\prod_{i=1}^n \\alpha_i^{y_i}(1-\\alpha_i)^{1-y_i}\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the label of the i-th sample $x_i$. The log-likelihood can be reformulated as\n",
    "\\begin{align}\n",
    "l({\\bf w})&=\\sum_{i=1}^n log \\; p_{y_i}({\\bf x_i};{\\bf w}) \\\\\n",
    "&= \\sum_{i=1}^n y_i log \\; \\alpha_i + (1-y_i)log(1-\\alpha_i) \\\\\n",
    "&= \\sum_{i=1}^n y_i \\left({\\bf w}^{T} {\\bf x_i} - log(1+e^{{\\bf w}^T {\\bf x_i}})\\right)+ (1-y_i)\\left(-log(1+e^{{\\bf w}^T {\\bf x_i}})\\right) \\\\\n",
    "&= \\sum_{i=1}^n \\lbrace y_i {\\bf w}^T {\\bf x_i} - log(1+e^{{\\bf w}^T {\\bf x_i}}) \\rbrace\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score equation\n",
    "Firstly, we can do some transformation for the coming computation.\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial w_j}log(\\alpha) = \\frac{\\partial}{\\partial w_j} \\sigma({\\bf w}^T {\\bf x}) = \\frac {e^{{\\bf w}^{T}{\\bf x}}\\;{x_j}} {1+e^{{\\bf w}^T {\\bf x}}} = x_j \\; (1-\\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "Similarly,  we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial w_j}log(1-\\alpha) = \\cdots = -x_j + x_j \\; (1-\\alpha) = -x_j\\alpha\n",
    "\\end{equation}\n",
    "\n",
    "Hence, \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_j} l({\\bf w}) = 0 &= \\sum_{i=1}^n y_i x_{ij} (1-\\alpha_i) - (1-y_i)(\\alpha_i x_{ij}) \\\\\n",
    "&= \\sum_{i=1}^n y_i x_{ij} - \\alpha_i x_{ij} \\\\\n",
    "&= \\sum_{i=1}^n (y_i - \\alpha_i)x_{ij} \\\\\n",
    "&= \\sum_{i=1}^n x_{ij}(y_i - p_{y_i}({\\bf x_i};{\\bf w}))\n",
    "\\end{align} \n",
    "\n",
    "Where ${\\bf x_i} = (x_{i1},x_{i2},\\cdots, x_{id})$, d represents the dimention of the ${\\bf x}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of w\n",
    "$ \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "{\\Bbb A}= \\left\\{ \\begin{matrix} x_{11} & x_{12} & \\cdots & x_{1d}\\\\ x_{21} & x_{22} & \\cdots & x_{2d}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{nd} \\end{matrix} \\right\\}\n",
    "$\n",
    "\n",
    "Fix j, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial w_j} l({\\bf w})  =  \\sum_{i=1}^n (y_i - \\alpha_i)x_{ij} \n",
    "\\end{equation} \n",
    "\n",
    "Write it in a metrix way, we obtain \n",
    "$\\quad\n",
    "g = \\nabla_{\\bf w} l = {\\Bbb A}^{T}(\\alpha-{\\bf y})\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${Compute \\; \\nabla_{\\bf w} \\;Vector \\\\\n",
    "输入参数w是d*1列向量，xdata是n*d矩阵,y是d*1的列向量} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.84115857],\n",
       "        [ 0.01331768],\n",
       "        [-0.3482957 ]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getGradient(w,xdata,y):\n",
    "    \n",
    "    alpha_mat = getAlphaMatrix(w,xdata)\n",
    "#     print(\"alpha_mat = \",alpha_mat)\n",
    "    return np.dot(np.transpose(xdata),(alpha_mat-y))\n",
    "\n",
    "getGradient(w,data,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${Compute \\; B_{\\bf w} \\;Matrix \\\\\n",
    "输入参数w是d*1列向量，xdata是n*d矩阵} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18736988, 0.        , 0.        ],\n",
       "       [0.        , 0.2139097 , 0.        ],\n",
       "       [0.        , 0.        , 0.20550031]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getWeightedMatrix(w,xdata):\n",
    "    \n",
    "    alpha_mat = getAlphaMatrix(w,xdata)\n",
    "    # print(\"alpha_mat = \",alpha_mat)\n",
    "    degree = len(xdata)\n",
    "    # print(\"degree = \",degree)\n",
    "    B = np.zeros((degree,degree))\n",
    "    for i in range(degree):\n",
    "        B[i][i] = float(alpha_mat[i][0])*(1-float(alpha_mat[i][0]))\n",
    "    return B\n",
    "\n",
    "getWeightedMatrix(w,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hessian matrix of $l({\\bf w})$\n",
    "\n",
    "From the gradient of ${\\bf w}$, let us take one more derivative of this gradient, we get\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial w_j\\partial w_k} l({\\bf w})  = - \\sum_{i=1}^n x_{ij} \\frac{\\partial}{\\partial w_k} \\alpha_i\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "In order to figure out the derivative, we consider\n",
    "\n",
    "\\begin{align}\n",
    "{\\partial log\\alpha} &= \\frac{\\partial \\alpha}{\\alpha} \\\\\n",
    "{\\partial \\alpha} &= \\alpha {\\partial log\\alpha} \\\\\n",
    "&= \\alpha x_j(1-\\alpha)\n",
    "\\end{align}\n",
    "where $j = \\lbrace 1,2,3,\\cdots,n \\rbrace $.\n",
    "\n",
    "\\begin{equation}\n",
    "\\implies \\frac{\\partial}{\\partial w_k} \\alpha_i = x_{ik}\\alpha_i(1-\\alpha_i)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "\\implies \\frac{\\partial}{\\partial w_j\\partial w_k} l({\\bf w}) &= - \\sum_{i=1}^n x_{ij} \\frac{\\partial}{\\partial w_k} \\alpha_i \\\\\n",
    "&= - \\sum_{i=1}^n  x_{ij}x_{ik}\\alpha_i(1-\\alpha_i) \\\\\n",
    "&= \\sum_{i=1}^{n} x_ix_i^{T} p_{y_i}({\\bf x_i};{\\bf w})(1-p_{y_i}({\\bf x_i};{\\bf w}))\\\\\n",
    "\\implies &=- {\\Bbb Z_j}^{T}{\\Bbb B}{\\Bbb Z_k}\n",
    "\\end{align}\n",
    "\\\\\n",
    "  \n",
    "Where ${\\Bbb Z_j} = (x_{1j},x_{2j},\\cdots,x_{nj})^{T},\\; {\\Bbb B} =  \\left\\{ \\begin{matrix} \\alpha_1(1-\\alpha_1) & 0 & \\cdots & 0\\\\ 0 & \\alpha_2(1-\\alpha_2) & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\alpha_n(1-\\alpha_n) \\end{matrix} \\right\\}$ \n",
    "\n",
    "Therefore, the Hessian matrix should be\n",
    "\n",
    "\\begin{align}\n",
    "{\\Bbb H} = \\nabla_{\\bf w}^{2} l &= {\\Bbb A}^{T}{\\Bbb B}{\\Bbb A} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${Compute \\; Hessian \\;Matrix \\\\\n",
    "输入参数w是d*1列向量，xdata是n*d矩阵} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.24850897, 1.84687947, 1.19542934],\n",
       "        [1.84687947, 3.45462107, 2.16853959],\n",
       "        [1.19542934, 2.16853959, 2.10573892]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getHessianMatrix(w,xdata):\n",
    "    alpha_mat = getAlphaMatrix(w,xdata)\n",
    "    B = getWeightedMatrix(w,xdata)\n",
    "    return np.dot(np.dot(np.transpose(xdata),B),xdata)\n",
    "\n",
    "getHessianMatrix(w,data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton-Raphson method approach\n",
    "Finally, apply newton method to solve the parameters ${\\bf w}$. There are three steps for solving it.\n",
    "\n",
    "  \n",
    " $(1).\\;Initialize \\;{\\bf w}={\\bf w^{(0)}}$\n",
    "  \n",
    "\n",
    "$(2).\\;At\\; the\\; k-th \\;step, {\\bf w}^{(k)} = {\\bf w}^{(k-1)} - {\\Bbb H}^{-1}\\lfloor_{{\\bf w}^{(k-1)}}\\quad·\\quad g$\n",
    "\n",
    "  \n",
    "$(3).\\; Once \\;|{\\bf w}^{k} - {\\bf w}^{k-1}|<\\epsilon, stop;\\; otherwise,\\; go\\; back\\; to \\;step (2).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtonMethod(n, w, xdata, y):\n",
    "\n",
    "    time = n\n",
    "    g = getGradient(w ,xdata ,y)\n",
    "    H = getHessianMatrix(w ,xdata)\n",
    "#     print(\"第{}次迭代，所得矩阵如下：\".format(time))\n",
    "#     print('g = ', g)\n",
    "#     print('H = ', H)\n",
    "#     print('w = ', w)\n",
    "\n",
    "    Next_w = w - np.dot(np.linalg.inv(H) ,g)\n",
    "    print(\"\\n第{}次迭代: \\n所得参数 w = {}\\n|Next_w - w| = {}.\".format(time,w.T,str(np.linalg.norm(w - Next_w))))\n",
    "\n",
    "    if np.linalg.norm(w - Next_w) < 1e-06:\n",
    "        print('\\n达到精度要求 |Next_w - w| = {} <1e-06 \\n '.format(str(np.linalg.norm(w - Next_w))))\n",
    "        '''设置迭代跳出条件'''\n",
    "        return Next_w\n",
    "    else:\n",
    "        return newtonMethod(n + 1, Next_w ,xdata ,y)\n",
    "\n",
    "    \n",
    "# newtonMethod(1, w, data, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\star}{\\star}{\\star}\\quad$ 算法检验 $\\quad{\\star}{\\star}{\\star}$\n",
    "Apply the program developed in the previous step to play with the South African\n",
    "Heart Disease data: https://sci2s.ugr.es/keel/dataset.php?cod=184. \n",
    "  \n",
    "We will use the copy of the data set already partitioned by means of 5-folds cross validation.\n",
    "This set of data, named “saheart-5-fold”, is also provided in the package of this\n",
    "project. Note that there are five groups of data, each of which consists of a training\n",
    "set (e.g. “saheart-5-1tra.dat”) and a test set (e.g. “saheart-5-1tst.dat”). Train the model by the training set in each group and test the model by the test set in the\n",
    "same group. Evaluate your results in terms of accuracy. (Hint: you have to do\n",
    "one-hot encoding for the non-numeric attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_dat(filename):\n",
    "\n",
    "    filename = filename\n",
    "    con_new = []\n",
    "    with open(filename,'r' ) as f:\n",
    "        contents = f.readlines()\n",
    "        contents = list(contents)\n",
    "        con_new.append([])  # 第一行列名\n",
    "\n",
    "        for i in contents[0:14]:\n",
    "            try:\n",
    "                attribute = re.findall(r'@attribute (.+?) ', i)[0]\n",
    "                con_new[0].append(attribute)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        del contents[:14]\n",
    "\n",
    "        for i,con in enumerate(contents):\n",
    "            tmp = con.strip()\n",
    "            tmp = tmp.split(',')\n",
    "            con_new.append([])\n",
    "            con_new[i+1].append(int(tmp[0]))\n",
    "            con_new[i+1].append(float(tmp[1]))\n",
    "            con_new[i+1].append(float(tmp[2]))\n",
    "            con_new[i+1].append(float(tmp[3]))\n",
    "            con_new[i+1].append(tmp[4])\n",
    "            con_new[i+1].append(int(tmp[5]))\n",
    "            con_new[i+1].append(float(tmp[6]))\n",
    "            con_new[i+1].append(float(tmp[7]))\n",
    "            con_new[i+1].append(int(tmp[8]))\n",
    "            con_new[i+1].append(int(tmp[9]))\n",
    "\n",
    "    df = pd.DataFrame(con_new[1:],columns=con_new[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def getXYtrain_test(df_train,df_test):\n",
    "\n",
    "    xtrain = pd.get_dummies(df_train.loc[:,'Sbp':'Age'], prefix=['Famhist'])\n",
    "    xtrain = np.array(xtrain)\n",
    "    ytrain = np.array(df_train.loc[:,'Chd'])\n",
    "\n",
    "    xtest = pd.get_dummies(df_test.loc[:,'Sbp':'Age'], prefix=['Famhist'])\n",
    "    xtest = np.array(xtest)\n",
    "    ytest = np.array(df_test.loc[:,'Chd'])\n",
    "\n",
    "    return xtrain,ytrain,xtest,ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第1次迭代: \n",
      "所得参数 w = [[0 0 0 0 0 0 0 0 0 0]]\n",
      "|Next_w - w| = 5.05990928344392.\n",
      "\n",
      "第2次迭代: \n",
      "所得参数 w = [[ 0.00519231  0.08222348  0.12660645  0.00983893  0.02371088 -0.04067755\n",
      "  -0.00416106  0.02377418 -3.92038342 -3.19492766]]\n",
      "|Next_w - w| = 2.0890090564818733.\n",
      "\n",
      "第3次迭代: \n",
      "所得参数 w = [[ 6.56226326e-03  9.71951372e-02  1.54683043e-01  1.87747702e-02\n",
      "   3.54037491e-02 -5.74322530e-02 -4.32493635e-03  3.56976702e-02\n",
      "  -5.48990242e+00 -4.57293896e+00]]\n",
      "|Next_w - w| = 0.5287401250365746.\n",
      "\n",
      "第4次迭代: \n",
      "所得参数 w = [[ 6.74586466e-03  9.96318295e-02  1.59298135e-01  2.04295943e-02\n",
      "   3.80947591e-02 -5.93631426e-02 -4.19598825e-03  3.89143867e-02\n",
      "  -5.88104957e+00 -4.92863360e+00]]\n",
      "|Next_w - w| = 0.02452977155602159.\n",
      "\n",
      "第5次迭代: \n",
      "所得参数 w = [[ 6.75035175e-03  9.97253195e-02  1.59469218e-01  2.04816372e-02\n",
      "   3.82045825e-02 -5.93831289e-02 -4.18678785e-03  3.90724239e-02\n",
      "  -5.89905749e+00 -4.94528733e+00]]\n",
      "|Next_w - w| = 4.758758989000873e-05.\n",
      "\n",
      "第6次迭代: \n",
      "所得参数 w = [[ 6.75035529e-03  9.97254809e-02  1.59469518e-01  2.04817163e-02\n",
      "   3.82047746e-02 -5.93831016e-02 -4.18676610e-03  3.90727468e-02\n",
      "  -5.89909230e+00 -4.94531977e+00]]\n",
      "|Next_w - w| = 1.7997039680636089e-10.\n",
      "\n",
      "达到精度要求 |Next_w - w| = 1.7997039680636089e-10 <1e-06 \n",
      " \n",
      "[[ 6.75035529e-03  9.97254809e-02  1.59469518e-01  2.04817163e-02\n",
      "   3.82047746e-02 -5.93831016e-02 -4.18676610e-03  3.90727468e-02\n",
      "  -5.89909230e+00 -4.94531977e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 主函数测试\n",
    "root_path = 'C:/Users/49732/Desktop/BigDataProjects/project-1/saheart-5-fold/'\n",
    "train_name = 'saheart-5-' + str(1) + 'tra.dat'\n",
    "test_name = 'saheart-5-' + str(1) + 'tst.dat'\n",
    "train_file_path = os.path.join(root_path,train_name)\n",
    "test_file_path = os.path.join(root_path,test_name)\n",
    "\n",
    "train_df = read_dat(train_file_path)\n",
    "test_df = read_dat(test_file_path)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = getXYtrain_test(train_df,test_df)\n",
    "\n",
    "w = np.array([0 ,0,0,0 ,0 ,0,0 ,0 ,0,0])\n",
    "w = np.mat(w)\n",
    "w = np.transpose(w)\n",
    "# print(w)\n",
    "data = np.mat(xtrain)\n",
    "y = np.mat(ytrain)\n",
    "y = np.transpose(y)\n",
    "# print(\"w = \\n\" ,w)\n",
    "# print(\"data = \\n\", data)\n",
    "# print(\"y = \\n\" ,y)\n",
    "final_w = newtonMethod(1, w, data, y)\n",
    "print(final_w.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据 final_w 计算出来的 $y_i$ = 1的概率 $\\quad{\\star}{\\star}{\\star}$\n",
    "$Pr(y=1|{\\bf X}={\\bf x}) = \\frac{e^{({\\bf w}_{final}^{'}\\;\\; x)}}{1+e^{({\\bf w}_{final}^{'}\\;\\; x)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7372125832925219, 0.35200585532216383, 0.2542132496512069, 0.648398787514912, 0.7242045343126333, 0.579594786858259, 0.2642487293904019, 0.6750559294160994, 0.13353252621109532, 0.5888423990217194]\n"
     ]
    }
   ],
   "source": [
    "predict = np.dot(final_w.T,np.transpose(np.mat(xtrain)))\n",
    "probility = (np.exp(predict.T)/(1+np.exp(predict.T))).T.tolist()[0]\n",
    "print(probility[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集判断模型准确度 $\\quad{\\star}{\\star}{\\star}$\n",
    "$模型准确度\\;Rate = \\frac{预测与真实相同的个数}{总样本量}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型训练集准确度 Rate =  0.7567567567567568\n"
     ]
    }
   ],
   "source": [
    "equal = 0 # 预测的分类与真实值相同的个数\n",
    "not_equal = 0 # 不同的个数\n",
    "for i,y in enumerate(list(ytrain)):\n",
    "    # 设定概率大于0.5，就分类为1，否则为0\n",
    "    if probility[i] >= 0.5:\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "    if c == y:\n",
    "        equal += 1\n",
    "    else:\n",
    "        not_equal += 1\n",
    "print(\"模型训练集准确度 Rate = \",equal/(equal+not_equal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集判断模型准确度 $\\quad{\\star}{\\star}{\\star}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集数据量： 92\n",
      "模型测试集准确度 Rate =  0.6847826086956522\n"
     ]
    }
   ],
   "source": [
    "# data = np.mat(xtest)\n",
    "# y = np.mat(ytest)\n",
    "# y = np.transpose(y)\n",
    "predict = np.dot(final_w.T,np.transpose(np.mat(xtest)))\n",
    "probility = (np.exp(predict.T)/(1+np.exp(predict.T))).T.tolist()[0]\n",
    "print(\"测试集数据量：\",len(probility)) \n",
    "\n",
    "equal = 0 # 预测的分类与真实值相同的个数\n",
    "not_equal = 0 # 不同的个数\n",
    "for i,y in enumerate(list(ytest)):\n",
    "    # 设定概率大于0.5，就分类为1，否则为0\n",
    "    if probility[i] >= 0.5:\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "    if c == y:\n",
    "        equal += 1\n",
    "    else:\n",
    "        not_equal += 1\n",
    "print(\"模型测试集准确度 Rate = \",equal/(equal+not_equal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与 sklearn 自带的 LogisticRegression 包算法作对比 $\\quad{\\star}{\\star}{\\star}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型准确度 Rate =  0.7371273712737128\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(xtrain, ytrain)\n",
    "print(\"模型准确度 Rate = \",clf.score(xtrain, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集混淆矩阵为：\n",
      " [[53  8]\n",
      " [18 14]]\n",
      "\n",
      "测试集准确度 Rate =  0.7204301075268817\n"
     ]
    }
   ],
   "source": [
    "test_predict = clf.predict(xtest)\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(ytest, test_predict)\n",
    "df_metrix = np.array(conf_matrix)\n",
    "conf_matrix = np.round(df_metrix, 2)\n",
    "print(\"测试集混淆矩阵为：\\n\",conf_matrix)\n",
    "\n",
    "true_rate = accuracy_score(ytest, test_predict)\n",
    "print(\"\\n测试集准确度 Rate = \",true_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn LogisticRegression在五个训练集和测试集上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn LogisticRegression在五个训练集和测试集上的表现:\n",
      "\n",
      "训练样本集1准确度为：0.737.\n",
      "训练样本集2准确度为：0.724.\n",
      "训练样本集3准确度为：0.735.\n",
      "训练样本集4准确度为：0.73.\n",
      "训练样本集5准确度为：0.757.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sklearn LogisticRegression在五个训练集和测试集上的表现:\\n\")\n",
    "for i in range(5):\n",
    "\n",
    "    train_name = 'saheart-5-' + str(i+1) + 'tra.dat'\n",
    "    test_name = 'saheart-5-' + str(i+1) + 'tst.dat'\n",
    "    train_file_path = os.path.join(root_path,train_name)\n",
    "    test_file_path = os.path.join(root_path,test_name)\n",
    "    train_df = read_dat(train_file_path)\n",
    "    test_df = read_dat(test_file_path)\n",
    "    xtrain, ytrain, xtest, ytest = getXYtrain_test(train_df,test_df)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    print('训练样本集{}准确度为：{}.'.format(i+1,round(clf.score(xtrain, ytrain),3)))\n",
    "#     print('测试样本集{}准确度为：{}.'.format(i+1,round(clf.score(xtest, ytest),3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  我们开发的算法在五个数据集上的表现：\n",
    "训练样本集1准确度为：0.7398373983739838    \n",
    "训练样本集2准确度为：0.7506775067750677  \n",
    "训练样本集3准确度为：0.7513513513513513   \n",
    "训练样本集4准确度为：0.7405405405405405  \n",
    "训练样本集5准确度为：0.7567567567567568   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\star}{\\star}{\\star}\\quad$由以上结果可以看出，Maximum likelihood approach for logistic regression 与 Sklearn 的效果基本一致，在Pycharm上测试速度，我的算法大概比自带的包速度快4倍，模型精度等各项指标均和自带的包类似 $\\quad{\\star}{\\star}{\\star}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\star}{\\star}{\\star}\\quad$完结 $\\quad{\\star}{\\star}{\\star}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
